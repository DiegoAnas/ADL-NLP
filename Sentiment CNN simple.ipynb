{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment CNN simple.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoAnas/SNN-NLP/blob/master/Sentiment%20CNN%20simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD2yitUiLcUw",
        "colab_type": "code",
        "outputId": "2d0ad89a-387e-4e48-e6ab-661328320d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQwTZ12FxVIo",
        "colab_type": "code",
        "outputId": "35e0f973-6685-45ad-fe8e-a0a3a3d528b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, concatenate, Input, \\\n",
        " Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJdtm3MvxZ1C",
        "colab_type": "code",
        "outputId": "a008ddbd-769c-4f9d-9d04-c25f9b05f078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Parameters:\n",
        "# Word Embedding\n",
        "max_features = 10000\n",
        "maxlen = 400\n",
        "embedding_dims = 250\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 3 # or filter length\n",
        "filters = 250 \n",
        "pool_size = 4 \n",
        "\n",
        "# Training\n",
        "batch_size = 30\n",
        "epochs = 2\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "x_data = np.concatenate((x_train, x_test))\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "skf = StratifiedKFold(n_splits=5, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYbKDTgxxqRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network Architecture\n",
        "def create_conv_model(activation:str, filter_length, batch_normalize:bool=False,\n",
        "                         dropout:bool=False, dropout_rate:float=0.1, embedding_dropout:bool=False, \n",
        "                         embedding_dropout_rate:float=0.1):\n",
        "  # Single branch CNN\n",
        "  var_input = Input(shape=(400,))\n",
        "  emb_layer = Embedding(max_features,\n",
        "                      embedding_dims,\n",
        "                      input_length=maxlen)(var_input)\n",
        "  if embedding_dropout:\n",
        "    emb_layer = SpatialDropout1D(rate=embedding_dropout_rate)(emb_layer)\n",
        "  conv_branches = []\n",
        "  conv_layer_1 = Conv1D(filters, kernel_size=filter_length, padding='valid', \n",
        "                        activation=activation, strides=1)(emb_layer)\n",
        "  if batch_normalize:\n",
        "        conv_layer_1 = BatchNormalization()(conv_layer_1)\n",
        "  maxpooling = GlobalMaxPooling1D()(conv_layer_1)\n",
        "  if dropout:\n",
        "      maxpooling = Dropout(rate=dropout_rate, seed=42)(maxpooling)\n",
        "  dense_layer = Dense(1, activation='sigmoid')(maxpooling)\n",
        "  model = Model(inputs=var_input, outputs=dense_layer)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXSrq3Ndk-ZH",
        "colab_type": "code",
        "outputId": "e30dafb8-11be-4904-a5fc-aa33118f6f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "#Test w ReLU\n",
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_model(activation='relu', filter_length=3)\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3136 - accuracy: 0.8623 - val_loss: 0.2400 - val_accuracy: 0.9032\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.1550 - accuracy: 0.9422 - val_loss: 0.2461 - val_accuracy: 0.9033\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3121 - accuracy: 0.8625 - val_loss: 0.2440 - val_accuracy: 0.9012\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.1497 - accuracy: 0.9462 - val_loss: 0.2607 - val_accuracy: 0.8967\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3151 - accuracy: 0.8586 - val_loss: 0.2347 - val_accuracy: 0.9050\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.1554 - accuracy: 0.9402 - val_loss: 0.2319 - val_accuracy: 0.9094\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3187 - accuracy: 0.8594 - val_loss: 0.2376 - val_accuracy: 0.9022\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.1548 - accuracy: 0.9421 - val_loss: 0.2425 - val_accuracy: 0.9006\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3180 - accuracy: 0.8584 - val_loss: 0.2227 - val_accuracy: 0.9103\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.1574 - accuracy: 0.9430 - val_loss: 0.2176 - val_accuracy: 0.9149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdWIrhL1mNCe",
        "colab_type": "code",
        "outputId": "4b86f64b-ff28-4669-d328-0d99e498f797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "#Test w SeLU\n",
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_model(activation='selu', filter_length=3)\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 44s 1ms/sample - loss: 0.3140 - accuracy: 0.8627 - val_loss: 0.2393 - val_accuracy: 0.9013\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 44s 1ms/sample - loss: 0.1524 - accuracy: 0.9427 - val_loss: 0.2442 - val_accuracy: 0.9051\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3091 - accuracy: 0.8626 - val_loss: 0.2603 - val_accuracy: 0.8912\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.1489 - accuracy: 0.9453 - val_loss: 0.2471 - val_accuracy: 0.9024\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3130 - accuracy: 0.8612 - val_loss: 0.2360 - val_accuracy: 0.9039\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.1535 - accuracy: 0.9424 - val_loss: 0.2296 - val_accuracy: 0.9092\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.3147 - accuracy: 0.8612 - val_loss: 0.2363 - val_accuracy: 0.9004\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 42s 1ms/sample - loss: 0.1545 - accuracy: 0.9421 - val_loss: 0.2375 - val_accuracy: 0.9053\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.3193 - accuracy: 0.8602 - val_loss: 0.2317 - val_accuracy: 0.9070\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 43s 1ms/sample - loss: 0.1531 - accuracy: 0.9424 - val_loss: 0.2346 - val_accuracy: 0.9090\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}