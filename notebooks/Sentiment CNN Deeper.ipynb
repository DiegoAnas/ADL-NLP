{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment CNN Deeper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoAnas/SNN-NLP/blob/master/Sentiment%20CNN%20Deeper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhEODsQrlT_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5fbee30-0af9-4ae2-a8d6-b1a212cc72b5"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVPODbEulZYQ",
        "colab_type": "code",
        "outputId": "449d03ac-1fde-42f3-d4e2-9347ae58e47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, \\\n",
        "  Dense, Activation, GlobalMaxPooling1D, Add, Flatten, concatenate, Dropout\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJdtm3MvxZ1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters:\n",
        "# Word Embedding\n",
        "max_features = 10000\n",
        "maxlen = 400\n",
        "embedding_dims = 250\n",
        "\n",
        "# Convolution\n",
        "kernel_sizes = [3, 4, 5]  # or filter length\n",
        "filters = 250 \n",
        "pool_size = 4 \n",
        "\n",
        "# Training\n",
        "batch_size = 30\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOy-jACvlqM2",
        "colab_type": "code",
        "outputId": "dfd52195-8296-4859-f90b-ab3f4ba45fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "x_data = np.concatenate((x_train, x_test))\n",
        "y_data = np.concatenate((y_train, y_test))\n",
        "skf = StratifiedKFold(n_splits=5, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYbKDTgxxqRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network Architecture\n",
        "def create_conv_mb_model(activation:str, batch_normalize:bool=False, dropout:bool=False,\n",
        "                         dropout_rate:float=0.1, embedding_dropout:bool=False,\n",
        "                         embedding_dropout_rate:float=0.1):\n",
        "  # Extra layer Multi-branch\n",
        "  var_input = Input(shape=(400,))\n",
        "  emb_layer = Embedding(max_features,\n",
        "                      embedding_dims,\n",
        "                      input_length=maxlen)(var_input)\n",
        "  if embedding_dropout:\n",
        "    emb_layer = SpatialDropout1D(rate=embedding_dropout_rate)(emb_layer)\n",
        "  conv_branches = []\n",
        "  for kernel_size in kernel_sizes:\n",
        "    conv_layer_1 = Conv1D(filters, kernel_size=kernel_size, padding='valid', \n",
        "                          activation=activation, strides=1)(emb_layer)\n",
        "    if batch_normalize:\n",
        "          conv_layer_1 = BatchNormalization()(conv_layer_1)\n",
        "    conv_layer_2 = Conv1D(filters, kernel_size=kernel_size, padding='valid',\n",
        "                          activation=activation, strides=1)(conv_layer_1)\n",
        "    if batch_normalize:\n",
        "          conv_layer_2 = BatchNormalization()(conv_layer_2)\n",
        "    maxpooling = GlobalMaxPooling1D()(conv_layer_2)\n",
        "    conv_branches.append(maxpooling)\n",
        "\n",
        "  merge_layer = concatenate(conv_branches)\n",
        "  if dropout:\n",
        "      merge_layer = Dropout(rate=dropout_rate, seed=42)(merge_layer)\n",
        "  dense_layer = Dense(1, activation='sigmoid')(merge_layer)\n",
        "  model = Model(inputs=var_input, outputs=dense_layer)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed2IWWRXmgZ_",
        "colab_type": "code",
        "outputId": "37d3ccee-adef-4f78-9717-5579a702b0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_mb_model(activation='relu')\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.3632 - accuracy: 0.8352 - val_loss: 0.3216 - val_accuracy: 0.8612\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2499 - accuracy: 0.8960 - val_loss: 0.3481 - val_accuracy: 0.8522\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1930 - accuracy: 0.9219 - val_loss: 0.2833 - val_accuracy: 0.8863\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1375 - accuracy: 0.9464 - val_loss: 0.3052 - val_accuracy: 0.8842\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.0855 - accuracy: 0.9681 - val_loss: 0.4039 - val_accuracy: 0.8747\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3582 - accuracy: 0.8349 - val_loss: 0.2913 - val_accuracy: 0.8754\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2431 - accuracy: 0.9000 - val_loss: 0.2883 - val_accuracy: 0.8783\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1818 - accuracy: 0.9276 - val_loss: 0.3129 - val_accuracy: 0.8766\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1293 - accuracy: 0.9510 - val_loss: 0.3391 - val_accuracy: 0.8810\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.0814 - accuracy: 0.9701 - val_loss: 0.4047 - val_accuracy: 0.8802\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3617 - accuracy: 0.8341 - val_loss: 0.2857 - val_accuracy: 0.8786\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2503 - accuracy: 0.8964 - val_loss: 0.3400 - val_accuracy: 0.8600\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1934 - accuracy: 0.9230 - val_loss: 0.3127 - val_accuracy: 0.8732\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1330 - accuracy: 0.9480 - val_loss: 0.3470 - val_accuracy: 0.8803\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.0836 - accuracy: 0.9685 - val_loss: 0.4015 - val_accuracy: 0.8766\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3611 - accuracy: 0.8349 - val_loss: 0.2936 - val_accuracy: 0.8713\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2475 - accuracy: 0.8972 - val_loss: 0.2778 - val_accuracy: 0.8835\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1919 - accuracy: 0.9227 - val_loss: 0.2819 - val_accuracy: 0.8889\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1325 - accuracy: 0.9483 - val_loss: 0.3099 - val_accuracy: 0.8872\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.0868 - accuracy: 0.9672 - val_loss: 0.3710 - val_accuracy: 0.8840\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3676 - accuracy: 0.8312 - val_loss: 0.2717 - val_accuracy: 0.8847\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2536 - accuracy: 0.8951 - val_loss: 0.2710 - val_accuracy: 0.8855\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2000 - accuracy: 0.9201 - val_loss: 0.2991 - val_accuracy: 0.8775\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1386 - accuracy: 0.9473 - val_loss: 0.2940 - val_accuracy: 0.8931\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.0842 - accuracy: 0.9687 - val_loss: 0.6083 - val_accuracy: 0.8381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHR-b-HvvzbZ",
        "colab_type": "code",
        "outputId": "621b8347-df5b-4f53-d6e4-f6eafeac3f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_mb_model(activation='selu')\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3520 - accuracy: 0.8420 - val_loss: 0.3099 - val_accuracy: 0.8675\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2662 - accuracy: 0.8911 - val_loss: 0.2830 - val_accuracy: 0.8840\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2110 - accuracy: 0.9165 - val_loss: 0.2909 - val_accuracy: 0.8875\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1586 - accuracy: 0.9395 - val_loss: 0.3746 - val_accuracy: 0.8753\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1264 - accuracy: 0.9519 - val_loss: 0.4094 - val_accuracy: 0.8837\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3476 - accuracy: 0.8453 - val_loss: 0.2910 - val_accuracy: 0.8785\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2576 - accuracy: 0.8957 - val_loss: 0.3177 - val_accuracy: 0.8797\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1998 - accuracy: 0.9198 - val_loss: 0.3480 - val_accuracy: 0.8665\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1606 - accuracy: 0.9392 - val_loss: 0.7038 - val_accuracy: 0.8089\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1263 - accuracy: 0.9518 - val_loss: 0.5236 - val_accuracy: 0.8568\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3516 - accuracy: 0.8416 - val_loss: 0.2847 - val_accuracy: 0.8794\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2605 - accuracy: 0.8939 - val_loss: 0.5992 - val_accuracy: 0.7770\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2101 - accuracy: 0.9169 - val_loss: 0.3580 - val_accuracy: 0.8696\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1616 - accuracy: 0.9371 - val_loss: 0.4023 - val_accuracy: 0.8711\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1318 - accuracy: 0.9510 - val_loss: 0.4025 - val_accuracy: 0.8768\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3528 - accuracy: 0.8412 - val_loss: 0.2841 - val_accuracy: 0.8794\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2556 - accuracy: 0.8957 - val_loss: 0.2763 - val_accuracy: 0.8862\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2080 - accuracy: 0.9177 - val_loss: 0.3528 - val_accuracy: 0.8676\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1746 - accuracy: 0.9334 - val_loss: 0.3568 - val_accuracy: 0.8823\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1360 - accuracy: 0.9499 - val_loss: 0.5900 - val_accuracy: 0.8424\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3554 - accuracy: 0.8397 - val_loss: 0.2915 - val_accuracy: 0.8751\n",
            "Epoch 2/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2602 - accuracy: 0.8931 - val_loss: 0.2636 - val_accuracy: 0.8912\n",
            "Epoch 3/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2101 - accuracy: 0.9171 - val_loss: 0.2871 - val_accuracy: 0.8892\n",
            "Epoch 4/5\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.1626 - accuracy: 0.9368 - val_loss: 0.3336 - val_accuracy: 0.8828\n",
            "Epoch 5/5\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.1281 - accuracy: 0.9520 - val_loss: 0.4586 - val_accuracy: 0.8671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WaplvbF43gif",
        "colab": {}
      },
      "source": [
        "# Network Architecture\n",
        "def create_conv_mb_model_2(activation:str, batch_normalize:bool=False, dropout:bool=False, \n",
        "                           dropout_rate:float=0.1, embedding_dropout:bool=False, \n",
        "                           embedding_dropout_rate:float=0.1):\n",
        "  # Extra layer Multi-branch w/o globalmaxpooling\n",
        "  var_input = Input(shape=(400,))\n",
        "  emb_layer = Embedding(max_features,\n",
        "                      embedding_dims,\n",
        "                      input_length=maxlen)(var_input)\n",
        "  if embedding_dropout:\n",
        "    emb_layer = SpatialDropout1D(rate=embedding_dropout_rate)(emb_layer)\n",
        "  conv_branches = []\n",
        "  for kernel_size in kernel_sizes:\n",
        "    conv_layer_1 = Conv1D(filters, kernel_size=kernel_size, padding='valid', \n",
        "                          activation=activation, strides=1)(emb_layer)\n",
        "    if batch_normalize:\n",
        "          conv_layer_1 = BatchNormalization()(conv_layer_1)\n",
        "    conv_layer_2 = Conv1D(filters, kernel_size=kernel_size, padding='valid',\n",
        "                          activation=activation, strides=1)(conv_layer_1)\n",
        "    if batch_normalize:\n",
        "          conv_layer_2 = BatchNormalization()(conv_layer_2)\n",
        "    maxpooling = MaxPooling1D(pool_size=3)(conv_layer_2)\n",
        "    flattened = Flatten()(maxpooling)\n",
        "    conv_branches.append(flattened)\n",
        "\n",
        "  merge_layer = concatenate(conv_branches)\n",
        "  if dropout:\n",
        "      merge_layer = Dropout(rate=dropout_rate, seed=42)(merge_layer)\n",
        "  dense_layer = Dense(1, activation='sigmoid')(merge_layer)\n",
        "  model = Model(inputs=var_input, outputs=dense_layer)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxnfZCFLW74H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhotpMSX6Te4",
        "colab_type": "code",
        "outputId": "09d7dadf-bcdc-49e2-8ce4-3ca07b31c142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_mb_model_2(activation='relu')\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3556 - accuracy: 0.8372 - val_loss: 0.2886 - val_accuracy: 0.8782\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2717 - accuracy: 0.8871 - val_loss: 0.2831 - val_accuracy: 0.8836\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3613 - accuracy: 0.8285 - val_loss: 0.3071 - val_accuracy: 0.8689\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2711 - accuracy: 0.8885 - val_loss: 0.2971 - val_accuracy: 0.8792\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3690 - accuracy: 0.8230 - val_loss: 0.2959 - val_accuracy: 0.8765\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2752 - accuracy: 0.8855 - val_loss: 0.2818 - val_accuracy: 0.8852\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.3615 - accuracy: 0.8286 - val_loss: 0.2857 - val_accuracy: 0.8799\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 50s 1ms/sample - loss: 0.2702 - accuracy: 0.8888 - val_loss: 0.2753 - val_accuracy: 0.8845\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.3601 - accuracy: 0.8330 - val_loss: 0.2805 - val_accuracy: 0.8809\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.2746 - accuracy: 0.8856 - val_loss: 0.2897 - val_accuracy: 0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMVaoB8S6Ty3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "e3adde25-c7e9-421f-bbf2-582761227fb5"
      },
      "source": [
        "for train, test in skf.split(x_data, y_data):\n",
        "  target = create_conv_mb_model_2(activation='selu')\n",
        "  target.fit(x_data[train], y_data[train],\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_data[test], y_data[test]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.5485 - accuracy: 0.7797 - val_loss: 0.7152 - val_accuracy: 0.8064\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 1.0188 - accuracy: 0.8130 - val_loss: 1.3654 - val_accuracy: 0.8054\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.5702 - accuracy: 0.7793 - val_loss: 0.7965 - val_accuracy: 0.7737\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.9231 - accuracy: 0.8176 - val_loss: 1.4665 - val_accuracy: 0.7924\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.5624 - accuracy: 0.7825 - val_loss: 0.8221 - val_accuracy: 0.7972\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.9780 - accuracy: 0.8138 - val_loss: 2.3962 - val_accuracy: 0.7332\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.5709 - accuracy: 0.7785 - val_loss: 0.7328 - val_accuracy: 0.7882\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 0.9826 - accuracy: 0.8160 - val_loss: 1.3440 - val_accuracy: 0.8056\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 52s 1ms/sample - loss: 0.5561 - accuracy: 0.7850 - val_loss: 0.8124 - val_accuracy: 0.7843\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 51s 1ms/sample - loss: 1.0363 - accuracy: 0.8069 - val_loss: 1.2518 - val_accuracy: 0.8021\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}